{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c41dd9d",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
       "    return false;\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c05257",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Tweets downloaded from NLTK (See [here](https://www.nltk.org/howto/twitter.html#corpus_reader) for more details).\n",
    "* Load tweets from json format and retrieve the data we want \n",
    "* Apply NLP on the tweets with the use of NLTK library to find out how the words used in positive and negative tweets are different\n",
    "    * Different tokenisers\n",
    "    * Different sets of stop words\n",
    "    * Handling emoticons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22169192",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Have a quick look of the data\n",
    "\n",
    "Have a look of the first line of the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91ccd29f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"contributors\": null, \"coordinates\": null, \"text\": \"#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\", \"user\": {\"time_zone\": \"Paris\", \"profile_background_image_url\": \"http://pbs.twimg.com/profile_background_images/784477066/7a8d261ef8d27f2bdf08fadac65bea7b.jpeg\", \"geo_enabled\": true, \"profile_image_url_https\": \"https://pbs.twimg.com/profile_images/567331322830413825/bqH6u2DO_normal.jpeg\", \"url\": \"http://t.co/iY2ZZAJY1Y\", \"profile_text_color\": \"000000\", \"entities\": {\"url\": {\"urls\": [{\"url\": \"http://t.co/iY2ZZAJY1Y\", \"indices\": [0, 22], \"expanded_url\": \"http://www.international.cci-paris-idf.fr\", \"display_url\": \"international.cci-paris-idf.fr\"}]}, \"description\": {\"urls\": [{\"url\": \"http://t.co/wGg73YM5yh\", \"indices\": [102, 124], \"expanded_url\": \"http://ow.ly/KwxhI\", \"display_url\": \"ow.ly/KwxhI\"}]}}, \"listed_count\": 296, \"statuses_count\": 12858, \"profile_sidebar_fill_color\": \"DDFFCC\", \"profile_sidebar_border_color\": \"FFFFFF\", \"name\": \"CCIParisExport\", \"followers_count\": 3683, \"location\": \"France\", \"lang\": \"fr\", \"profile_background_image_url_https\": \"https://pbs.twimg.com/profile_background_images/784477066/7a8d261ef8d27f2bdf08fadac65bea7b.jpeg\", \"verified\": false, \"notifications\": false, \"default_profile_image\": false, \"profile_use_background_image\": true, \"created_at\": \"Thu Feb 18 13:18:49 +0000 2010\", \"favourites_count\": 1089, \"friends_count\": 1505, \"has_extended_profile\": false, \"utc_offset\": 7200, \"profile_background_color\": \"CEDFE9\", \"is_translator\": false, \"contributors_enabled\": false, \"profile_image_url\": \"http://pbs.twimg.com/profile_images/567331322830413825/bqH6u2DO_normal.jpeg\", \"profile_link_color\": \"02437B\", \"is_translation_enabled\": false, \"profile_banner_url\": \"https://pbs.twimg.com/profile_banners/115376341/1431008625\", \"id_str\": \"115376341\", \"follow_request_sent\": false, \"screen_name\": \"CCIParis_Export\", \"following\": false, \"default_profile\": false, \"profile_background_tile\": false, \"id\": 115376341, \"description\": \"D\\u00e9veloppez votre #Entreprise \\u00e0 l\\u2019#Export avec votre #CCI ! Vous pr\\u00e9f\\u00e9rez Linkedin ? Connectons-nous ! http://t.co/wGg73YM5yh #Commerce #International\", \"protected\": false}, \"retweet_count\": 0, \"favorited\": false, \"entities\": {\"hashtags\": [{\"indices\": [0, 13], \"text\": \"FollowFriday\"}], \"user_mentions\": [{\"screen_name\": \"France_Inte\", \"indices\": [14, 26], \"id_str\": \"3222273608\", \"id\": 3222273608, \"name\": \"France International\"}, {\"screen_name\": \"PKuchly57\", \"indices\": [27, 37], \"id_str\": \"164313583\", \"id\": 164313583, \"name\": \"Pierre KUCHLY\"}, {\"screen_name\": \"Milipol_Paris\", \"indices\": [38, 52], \"id_str\": \"1100740914\", \"id\": 1100740914, \"name\": \"Milipol Paris\"}], \"urls\": [], \"symbols\": []}, \"source\": \"<a href=\\\"https://commun.it\\\" rel=\\\"nofollow\\\">Commun.it</a>\", \"truncated\": false, \"geo\": null, \"in_reply_to_status_id_str\": null, \"is_quote_status\": false, \"in_reply_to_user_id_str\": null, \"place\": null, \"in_reply_to_status_id\": null, \"in_reply_to_screen_name\": null, \"lang\": \"en\", \"retweeted\": false, \"in_reply_to_user_id\": null, \"created_at\": \"Fri Jul 24 08:23:36 +0000 2015\", \"metadata\": {\"iso_language_code\": \"en\", \"result_type\": \"recent\"}, \"favorite_count\": 0, \"id_str\": \"624495129303351296\", \"id\": 624495129303351296}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('data/positive_tweets.json') as f:\n",
    "    for line in f:\n",
    "        print(line)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a48c55",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Load in the data using json\n",
    "\n",
    "Load in the data from json to a `list` of `dict` for both `'positive_tweets.json` and `negative_tweets.json`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ff96be0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "pos_tweets = []\n",
    "with open('data/positive_tweets.json') as f:\n",
    "    for line in f:\n",
    "        pos_tweets.append(json.loads(line))\n",
    "        \n",
    "neg_tweets = []\n",
    "with open('data/negative_tweets.json') as f:\n",
    "    for line in f:\n",
    "        neg_tweets.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ed93e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f938bb4d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Have a look of some example tweets\n",
    "\n",
    "Have a look of the text of the positive tweet at index 116 and negative tweet at index 57:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d71fc5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code here (store the positive tweet in example_pos_tweet and the negative tweet in example_neg_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad54d140",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Extract the text from tweets\n",
    "\n",
    "Store the text of each tweet to two lists, one for positive tweets, another for negative tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "425bad32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#write your code here (hint: loop through pos_tweets and neg_tweets and append it to pos_text and neg_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70cef42",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Simple data exploration using pandas\n",
    "\n",
    "We first put all tweets into one `DataFrame`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a0886d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pos_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m pos_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(pos_text, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      4\u001b[0m pos_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentiment\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpos\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      5\u001b[0m neg_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(neg_text, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pos_text' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pos_df = pd.DataFrame(pos_text, columns=['text'])\n",
    "pos_df['sentiment'] = 'pos'\n",
    "neg_df = pd.DataFrame(neg_text, columns=['text'])\n",
    "neg_df['sentiment'] = 'neg'\n",
    "\n",
    "df = pd.concat([pos_df, neg_df], axis = 0).reset_index(drop=True)\n",
    "df['sentiment'] = df['sentiment'].astype('category')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ef4859",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Simple data exploration using pandas (continue)\n",
    "\n",
    "Some positive tweets (with the use of `sample()`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ed5b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.loc[df.sentiment == 'pos', 'text'].sample(5).values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88176717",
   "metadata": {},
   "source": [
    "And some negative tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72d362bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Too hot :('\n",
      " '@AhamSharmaFC ohh so sad :( @StarPlus @FCManmarzian @ManmarzianFC'\n",
      " \"@biobio1993 Don't worry! It's understandable. :( I mean, I totally get why some people block others &amp; that's their choice but sometimes\"\n",
      " 'Back home :( (@ Rize Meydan) https://t.co/raRIqmgCJJ' 'So cold :(']\n"
     ]
    }
   ],
   "source": [
    "print(df.loc[df.sentiment == 'neg', 'text'].sample(5).values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d484999",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Simple data exploration using pandas (continue)\n",
    "\n",
    "Count the number of words used in tweets (in a naive way):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0c75dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new column \"tweet length\" (hint: use df[\"text\"].str.split().str.len())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe4b8bf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Simple data exploration using pandas (continue)\n",
    "\n",
    "Here we want to check whether we have a similar amount of positive and negative tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedce0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn and matplotlib.pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204a3b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use sns.countplot and set x=df[\"sentiment\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ce52a6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Simple data exploration using pandas (continue)\n",
    "\n",
    "See if there is a noticeable difference in terms of the length of tweets (so that we can use it directly to distinguish positive and negative tweets):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c41f21-092a-4e54-bd9b-d16af0b3dc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use fig,ax = plt.subplots() to create two side by side plots with figsize = (15,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92506365-ca71-4310-9cca-e5833a31585e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the left plot with sns.kdeplot (hint: set x = 'tweet length', hue = 'sentiment', ax = ax[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d91a966-9a98-4c3b-ad44-8f11ab4dad0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the right plot with sns.boxplot (hint: y = 'tweet length', x = 'sentiment', ax = ax[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00034bba",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Implement NLTK libraries\n",
    "\n",
    "Code is written differently to make it easier to modify the preprocessing steps later. Note here we do _not_ strip the punctuations - we want to keep the emoticons (at least for now)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bdb7eabd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pos_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstem\u001b[39m(tokens):\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [stemmer\u001b[38;5;241m.\u001b[39mstem(one_token) \u001b[38;5;28;01mfor\u001b[39;00m one_token \u001b[38;5;129;01min\u001b[39;00m tokens]\n\u001b[1;32m---> 22\u001b[0m all_text \u001b[38;5;241m=\u001b[39m pos_text \u001b[38;5;241m+\u001b[39m neg_text \u001b[38;5;66;03m# to combine positive and negative tweets\u001b[39;00m\n\u001b[0;32m     23\u001b[0m all_cleaned_tokens \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m all_text:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pos_text' is not defined"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tag import pos_tag\n",
    "import collections\n",
    "\n",
    "nltk_sw = stopwords.words('english')\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "def simple_tokenise(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "def to_lower(tokens):\n",
    "    return [one_token.lower() for one_token in tokens]\n",
    "\n",
    "def remove_nltk_sw(tokens):\n",
    "    return [one_token for one_token in tokens if one_token not in nltk_sw]\n",
    "\n",
    "def stem(tokens):\n",
    "    return [stemmer.stem(one_token) for one_token in tokens]\n",
    "\n",
    "all_text = pos_text + neg_text # to combine positive and negative tweets\n",
    "all_cleaned_tokens = []\n",
    "for text in all_text:\n",
    "    tokens = simple_tokenise(text)\n",
    "    tokens = to_lower(tokens)\n",
    "    tokens = remove_nltk_sw(tokens)\n",
    "    tokens = stem(tokens)\n",
    "    all_cleaned_tokens.append(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61ac844",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Find the most commen words in all tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df361cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = collections.Counter()\n",
    "for cleaned_tokens in all_cleaned_tokens:\n",
    "    word_count += collections.Counter(cleaned_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47a2846",
   "metadata": {},
   "source": [
    "Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1064c8-2cf8-4d5a-b2a5-b3cee55337b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the 20 most common words using word_count.most_common(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4b4c12",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Visualise the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf10a6e-2e3b-4168-9966-f22c3ad1fa8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe called count_df with pd.DataFrame() and set columns= ['token', 'freq']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a075bb-64ab-4972-a4e4-ca104330e635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph with sns.barplot() and set plt.xticks(rotation = 45)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8081a2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Compare  `word_tokenize()` with `TweetTokenizer().tokenize()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1955bb2-6609-4adf-bd75-b966ba02af3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the tokenized list for example_neg_tweet wtih word_tokenize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638027ae-e44e-4b6e-825a-a6980c5ef045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the tokenized list for example_neg_tweet wtih TweetTokenizer().tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be255406",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Use  `TweetTokenizer` \n",
    "\n",
    "Process the data again and count the tokens, but this time we use `TweetTokenizer` for tokenisation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "743f2fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_tokenise(text):\n",
    "    return TweetTokenizer().tokenize(text)\n",
    "\n",
    "all_cleaned_tokens = []\n",
    "for text in all_text:\n",
    "    tokens = tweet_tokenise(text)\n",
    "    tokens = to_lower(tokens)\n",
    "    tokens = remove_nltk_sw(tokens)\n",
    "    tokens = stem(tokens)\n",
    "    all_cleaned_tokens.append(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e4f5de",
   "metadata": {},
   "source": [
    "Try it on an example tweet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b1edef66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@Cjlopez21 you know I will ðŸ’ª Monica and I miss you to, yeah sounds good to me :)\n",
      "['@cjlopez21', 'know', 'ðŸ’ª', 'monica', 'miss', ',', 'yeah', 'sound', 'good', ':)']\n"
     ]
    }
   ],
   "source": [
    "print(example_pos_tweet)\n",
    "print(all_cleaned_tokens[example_pos_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f946fb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Use  `TweetTokenizer`  (continue)\n",
    "\n",
    "Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cf2638dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(':(', 4586), (':)', 3693), ('!', 2675), ('.', 2433), (',', 1698), ('(', 1262), ('?', 1225), ('thank', 751), ('follow', 706), (':-)', 701), (':d', 658), ('...', 622), (')', 584), ('love', 549), (\"i'm\", 526)]\n"
     ]
    }
   ],
   "source": [
    "word_count = collections.Counter()\n",
    "for cleaned_tokens in all_cleaned_tokens:\n",
    "    word_count += collections.Counter(cleaned_tokens)\n",
    "    \n",
    "print(word_count.most_common(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8dd89cb",
   "metadata": {},
   "source": [
    "Now we get a much better result - For example, now emoticons are intact."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc29747",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Find the most occurring words for positive and negative tweets\n",
    "\n",
    "Convert the code to tokenise and clean each tweet to a function for easy reuse later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2cb500ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cleaned_tokens_v0(text):\n",
    "    tokens = tweet_tokenise(text)\n",
    "    tokens = to_lower(tokens)\n",
    "    tokens = remove_nltk_sw(tokens)\n",
    "    tokens = stem(tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22edf032",
   "metadata": {},
   "source": [
    "Example use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "afc2f671",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@cjlopez21', 'know', 'ðŸ’ª', 'monica', 'miss', ',', 'yeah', 'sound', 'good', ':)']\n",
      "['#climatechang', '#cc', 'idaho', 'restrict', 'fish', 'despit', 'region', 'drought-link', 'die-of', '...', 'http://t.co/jjbodo6lyz', '#uniteblu', '#tcot', ':-(']\n"
     ]
    }
   ],
   "source": [
    "example_pos_tokens = get_cleaned_tokens_v0(example_pos_tweet)\n",
    "example_neg_tokens = get_cleaned_tokens_v0(example_neg_tweet)\n",
    "print(example_pos_tokens)\n",
    "print(example_neg_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2373b16-5745-4d4c-af6e-50d35d139c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create example_pos_tokens by applying get_cleaned_tokens_v0 on example_pos_tweet and print it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604bd9ef-1a8c-48b6-a991-59c19217f250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create example_neg_tokens by applying get_cleaned_tokens_v0 on example_neg_tweet and print it out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181f2e24",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Find the most occurring words for positive and negative  (continue)\n",
    "\n",
    "Convert the code to count the appearances of each token into a function for easy reuse later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "146b05df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_count(list_of_tokens):\n",
    "    word_count = collections.Counter()\n",
    "    for tokens in list_of_tokens:\n",
    "        word_count += collections.Counter(tokens)\n",
    "    return word_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d789d538",
   "metadata": {},
   "source": [
    "Example use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a53fbe79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'@cjlopez21': 1, 'know': 1, 'ðŸ’ª': 1, 'monica': 1, 'miss': 1, ',': 1, 'yeah': 1, 'sound': 1, 'good': 1, ':)': 1, '#climatechang': 1, '#cc': 1, 'idaho': 1, 'restrict': 1, 'fish': 1, 'despit': 1, 'region': 1, 'drought-link': 1, 'die-of': 1, '...': 1, 'http://t.co/jjbodo6lyz': 1, '#uniteblu': 1, '#tcot': 1, ':-(': 1})\n"
     ]
    }
   ],
   "source": [
    "example_list_of_tokens = [example_pos_tokens, example_neg_tokens]\n",
    "print(get_count(example_list_of_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc04fb7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Find the most occurring words for positive and negative  (continue)\n",
    "\n",
    "Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5737d5b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(':)', 3691), ('!', 1844), ('.', 1341), (',', 964), (':-)', 701), (':d', 658), ('thank', 644), ('?', 581), (')', 525), ('follow', 443), ('love', 398), ('...', 290), ('\"', 264), (':', 249), ('day', 245), ('u', 245), ('good', 238), ('like', 232), ('-', 213), ('get', 209)]\n"
     ]
    }
   ],
   "source": [
    "pos_tokens = [get_cleaned_tokens_v0(text) for text in pos_text]\n",
    "print(get_count(pos_tokens).most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c8a84035",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(':(', 4585), ('(', 1180), ('.', 1092), ('!', 831), (',', 734), ('?', 644), (':-(', 501), (\"i'm\", 343), ('...', 332), ('miss', 301), ('pleas', 275), ('follow', 263), ('want', 246), ('get', 233), ('go', 224), ('like', 223), ('\"', 215), (':', 211), ('â™›', 210), ('ã€‹', 210)]\n"
     ]
    }
   ],
   "source": [
    "neg_tokens = [get_cleaned_tokens_v0(text) for text in neg_text]\n",
    "print(get_count(neg_tokens).most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e9d914",
   "metadata": {},
   "source": [
    "There are nearly 4600 `:(` out of 5000 tweets! Are you surprised by the result? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b458cb9c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Note on the data\n",
    "\n",
    "If you read the `README.txt` file in the data folder, you can find out that the \"positive\" tweets were collected by searching against the following set of emoticons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8f012380",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_emoticon = set([':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n",
    "':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
    "'=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n",
    "'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)', '<3'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1234583",
   "metadata": {},
   "source": [
    "And the \"negative\" tweets were collected by searching against the following set of emoticons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "93647920",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_emoticon = set([':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
    "':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
    "':c', ':{', '>:\\\\', ';('])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8f8b6f",
   "metadata": {},
   "source": [
    "Therefore, due to how the data is collected, emoticons `:(` and `:)` appear a lot in the data. Using them directly we will be able to identify whether a given tweet is positive in the set of data. But this result will not generalise well to the real world data. \n",
    "\n",
    "Follow up question: Do you think the positive tweets are really \"positive\", and negative ones are really \"negative\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dedfb76",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Improvement 1: remove punctuations and emoticons\n",
    "\n",
    "Here we remove emoticons as we want to discover other insights from the text, rather than the \"self-fulfilling\" high number of appearance of emoticons. For simplicity we remove _all_ punctuations, although punctuations like `!` can be useful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3f8fc04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "all_emoticon = pos_emoticon.union(neg_emoticon)\n",
    "\n",
    "def remove_emoticon(tokens):\n",
    "    return [one_token for one_token in tokens if one_token not in all_emoticon]\n",
    "\n",
    "def remove_punctuation(tokens):\n",
    "    # remove right most punctations (as we do not want to remove the punctuations from #xxx @yyy)\n",
    "    return [one_token.rstrip(string.punctuation) for one_token in tokens if \n",
    "            len(one_token.rstrip(string.punctuation))]\n",
    "\n",
    "def get_cleaned_tokens_v1(text):\n",
    "    # remove emoticon and punction\n",
    "    # note the order of the pre-processing tasks is important\n",
    "    tokens = tweet_tokenise(text)\n",
    "    tokens = remove_emoticon(tokens)\n",
    "    tokens = remove_punctuation(tokens)\n",
    "    tokens = to_lower(tokens)\n",
    "    tokens = remove_nltk_sw(tokens)\n",
    "    tokens = stem(tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8649fa82",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Note on order of pre-processing tasks\n",
    "\n",
    "Order of pre-processing tasks is important. \n",
    "* Consider you have some emoticons like `:D` and `:o)`, some stop words like `To` in the text.\n",
    "    * If you perform `to_lower()` -> `remove_emoticon()`, you will turn `:D` to `:d` before trying to remove the emoticon\n",
    "    * If you perform `remove_punctuation()` -> `remove_emoticon()`, you will turn `:o)` to `:o` before trying to remove the emoticon\n",
    "    * If you perform `remove_nltk_sw()` -> `to_lower()`, you will not able to remove the stop word `To` as only `to` is in the list of stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c9394d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Improvement 1: remove punctuations and emoticons (continue)\n",
    "Exmple use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "95e22e2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@cjlopez21', 'know', 'ðŸ’ª', 'monica', 'miss', 'yeah', 'sound', 'good']\n",
      "['#climatechang', '#cc', 'idaho', 'restrict', 'fish', 'despit', 'region', 'drought-link', 'die-of', 'http://t.co/jjbodo6lyz', '#uniteblu', '#tcot']\n"
     ]
    }
   ],
   "source": [
    "print(get_cleaned_tokens_v1(example_pos_tweet))\n",
    "print(get_cleaned_tokens_v1(example_neg_tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d392bc",
   "metadata": {},
   "source": [
    "Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66339bf-4137-4381-9872-1a3ffdff6429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list called pos_tokens by looping through pos_text and appending get_cleaned_tokens_v1(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fad20b3-3ad6-4ded-9296-96abbf799741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the 20 most common tokens with get_count().most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc27412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list called neg_tokens by looping through neg_text and appending get_cleaned_tokens_v1(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5459710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the 20 most common tokens with get_count().most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500cfb19",
   "metadata": {},
   "source": [
    "No more emoticons and standalone punctuations in the result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5225b36d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Improvement 2: customise stop words\n",
    "\n",
    "Have a look of the stop words provided by NLTK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4ef6e2c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(nltk_sw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63bfba1d",
   "metadata": {},
   "source": [
    "For topic finding, words like \"not\", \"no\", \"don't\" may not be useful and we may want to filter them out. For what we are doing here, however, those \"negative stop words\" can be useful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7487b581",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Improvement 2: customise stop words (continue)\n",
    "\n",
    "Here we create a new set of stop words by only keeping the non-negative stop words from the list of stops words from NLTK, and also adding 2 more stop words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1d2af32e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'how', 'under', 'it', 'these', \"it's\", 'your', 'which', 'on', 'their', 'while', 'for', 'whom', 'you', 'some', 'been', 'any', 'what', \"she's\", 'he', 'over', 'off', 'as', 'myself', 'with', 'yourself', 'those', 'but', 'where', 'this', 'or', 'in', 'ours', 'y', 'more', 'be', 'out', 'she', 'her', 'a', 'then', 'own', 'very', 'yours', 'into', 'further', 'hers', 'themselves', 'when', 'was', 'because', \"should've\", \"i'm\", 's', 'yourselves', 'who', 'that', 'should', 'o', 'during', 'too', 'at', 'theirs', 'by', 'before', 'has', 'again', 'ourselves', 'them', 'they', 'the', 'both', 'each', 'here', 'down', 'are', 'only', 'being', 'll', 'have', 'after', 've', 'above', 'himself', 'from', 'our', 'if', 'had', 'herself', \"that'll\", 'through', 'm', 'there', 'once', 'd', 'most', 'me', 'about', 'do', 'other', 'having', 'why', 'him', 'than', 'few', 'all', 'so', 'up', 'am', 'were', 'same', 'will', 'just', 'i', 'can', 'did', 'doing', 'such', \"you've\", 'his', 'of', \"you'll\", 'and', 'now', 'until', 're', 'itself', 'an', 'u', 'we', \"you'd\", 'does', 'to', 'my', \"you're\", 'its', 'is', 'between'}\n"
     ]
    }
   ],
   "source": [
    "neg_sw = set(['against', 'below', 'no', 'nor', 'not', 'don', \"don't\", 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\",\n",
    "          'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \n",
    "          \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn',\n",
    "          \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", 't'])\n",
    "my_sw = set(nltk_sw) - neg_sw\n",
    "my_sw.add('u')\n",
    "my_sw.add(\"i'm\")\n",
    "print(my_sw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d91d9e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Improvement 2: customise stopwords (continue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ee08e321",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_my_sw(tokens):\n",
    "    return [one_token for one_token in tokens if one_token not in my_sw]\n",
    "    \n",
    "def get_cleaned_tokens_v2(text):\n",
    "    # now we use our own set of stopwords\n",
    "    tokens = tweet_tokenise(text)\n",
    "    tokens = remove_emoticon(tokens) \n",
    "    tokens = remove_punctuation(tokens)\n",
    "    tokens = to_lower(tokens)\n",
    "    tokens = remove_my_sw(tokens)\n",
    "    tokens = stem(tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c545c96a",
   "metadata": {},
   "source": [
    "Example use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88082e2e-b31b-4844-9ff5-f156ba74c072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the cleaned tokens for example_pos_tweet and example_neg_tweet with get_cleaned_tokens_v2 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab61bb2",
   "metadata": {},
   "source": [
    "\"not\" is now kept for the second example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd611a52",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9a7c9c09",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('thank', 644), ('follow', 443), ('love', 398), ('day', 245), ('good', 238), ('like', 232), ('get', 209), ('happi', 206), ('see', 186), ('great', 173), ('hi', 173), ('no', 167), ('back', 163), ('know', 155), ('go', 151), ('new', 146), ('hope', 143), ('not', 142), ('look', 140), ('one', 131)]\n"
     ]
    }
   ],
   "source": [
    "pos_tokens = [get_cleaned_tokens_v2(text) for text in pos_text]\n",
    "print(get_count(pos_tokens).most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5fc7534f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('miss', 301), ('not', 300), ('no', 284), ('pleas', 275), ('follow', 263), ('want', 246), ('get', 233), ('go', 224), ('like', 223), ('â™›', 210), ('ã€‹', 210), (\"can't\", 180), (\"don't\", 176), ('time', 166), ('feel', 158), ('love', 151), ('day', 150), ('one', 150), ('sorri', 149), ('much', 139)]\n"
     ]
    }
   ],
   "source": [
    "neg_tokens = [get_cleaned_tokens_v2(text) for text in neg_text]\n",
    "print(get_count(neg_tokens).most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8ec180",
   "metadata": {},
   "source": [
    "Note:\n",
    "* Now the \"negative\" stop words `no`, `not`, `don't` are kept, and they appear more frequently in negative tweets than the positive ones\n",
    "* Why there are a lot of `â™›` and `ã€‹`?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95dbecd3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Quick investigation on 'â™›' and 'ã€‹'\n",
    "\n",
    "Have a look of the tweets with those symbols:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "31ba440b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â™›â™›â™›\n",
      "ã€‹ã€‹ã€‹ã€‹ \n",
      "I LOVE YOU SO MUCH.\n",
      "I BELÄ°EVE THAT HE WÄ°LL FOLLOW.\n",
      "PLEASE FOLLOW ME PLEASE JUSTÄ°N @justinbieber :( x15.350\n",
      "ã€‹ã€‹ã€‹ã€‹ï¼³ï¼¥ï¼¥ ï¼­ï¼¥\n",
      "â™›â™›â™›\n"
     ]
    }
   ],
   "source": [
    "for text in neg_text:\n",
    "    if 'â™›' in text or 'ã€‹' in text:\n",
    "        print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c764d02",
   "metadata": {},
   "source": [
    "Spam tweets? You may want to remove them by considering interactions like number of comments, likes and share. \n",
    "* In this example, some words like \"please\" and \"follow\" seems to associate with spam tweets as well\n",
    "\n",
    "(Unfortunately, when collecting data, it is common you will have (a fairly large amount of) spam tweets.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbc9fbf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Results from the tweet exploration\n",
    "\n",
    "* Words that often associated with positive sentiments like `thanks`, `love`, `good`, `happi` (happy?), `great` and `hope` are more likely to be seen in the \"positive\" tweets\n",
    "* Words that are often associated with negative sentiments like `miss`, `not`, `no`, `can't`, `don't` and `sorri` (sorry?) are more likely to be seen in the \"negative\" tweets\n",
    "* The unusual large number of appearances of characters like `â™›` and `ã€‹` leads us to discover some spam tweets in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3227284",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What we have learned from this simple tweet exploration?\n",
    "\n",
    "Related to text analysis:\n",
    "* Which pre-processing tasks to do depends on the types of data we have. For example:\n",
    "    * For tweets, considering punctuations as separate tokens is not ideal due to the frequent use of `#xxx`, `@yyy`, urls, emoticons\n",
    "    * When comparing different sentiments, keeping \"negative\" \"stop words\" can be useful\n",
    "* Order of pre-processing tasks are important\n",
    "* Simple text exploration allows us to get some insights from the data and can be useful to help us to find out issues with the quality of data\n",
    "    * Spam tweets in this case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164f82e5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What we have learned from this simple tweet exploration? (continue)\n",
    "\n",
    "* Reading meta data before analysing data is important\n",
    "    * It provides you important details about the data\n",
    "    * It saves you time to \"rediscover\" things already mentioned in the meta data\n",
    "        * e.g. \"discovering\" sad face is frquently used in \"negative\" tweets\n",
    "    * It prevents you to misunderstood and misinterpret what data \n",
    "        * e.g. \"negative tweets\" here are tweets with negative emoticons, NOT necessarily tweets with negative sentiment\n",
    "* When collecting tweets (or other social media posts), it is likely you will have some spam posts that you may need to filter them out to have some useful results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234c71b2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Further cleaning\n",
    "\n",
    "Depending on what you want to achieve, (e.g. if you want to feed the processed text to some models) you may want to remove @xxxx and url as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "58f407c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_start_with(tokens, prefix):\n",
    "    return [one_token for one_token in tokens if not one_token.startswith(prefix)]\n",
    "    \n",
    "def get_cleaned_tokens_v3(text):\n",
    "    tokens = tweet_tokenise(text)\n",
    "    tokens = remove_start_with(tokens, '@')\n",
    "    tokens = remove_start_with(tokens, 'http')\n",
    "    tokens = remove_emoticon(tokens)\n",
    "    tokens = remove_punctuation(tokens)\n",
    "    tokens = to_lower(tokens)\n",
    "    tokens = remove_my_sw(tokens)\n",
    "    tokens = stem(tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab357f9",
   "metadata": {},
   "source": [
    "Example use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0de7cc18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['know', 'ðŸ’ª', 'monica', 'miss', 'yeah', 'sound', 'good']\n",
      "['#climatechang', '#cc', 'idaho', 'not', 'restrict', 'fish', 'despit', 'region', 'drought-link', 'die-of', '#uniteblu', '#tcot']\n"
     ]
    }
   ],
   "source": [
    "pos_tokens = [get_cleaned_tokens_v3(text) for text in pos_text]\n",
    "neg_tokens = [get_cleaned_tokens_v3(text) for text in neg_text]\n",
    "print(pos_tokens[example_pos_idx])\n",
    "print(neg_tokens[example_neg_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863d49d7",
   "metadata": {},
   "source": [
    "Notice now @xxx and some url like http://.... are removed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965d6869",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Further cleaning (continue)\n",
    "\n",
    "Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c326cb68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('thank', 644), ('follow', 443), ('love', 398), ('day', 245), ('good', 238), ('like', 232), ('get', 209), ('happi', 206), ('see', 186), ('great', 173), ('hi', 173), ('no', 167), ('back', 163), ('know', 155), ('go', 151), ('new', 146), ('hope', 143), ('not', 142), ('look', 140), ('one', 131)]\n"
     ]
    }
   ],
   "source": [
    "print(get_count(pos_tokens).most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7b6d5acf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('miss', 301), ('not', 300), ('no', 284), ('pleas', 275), ('follow', 263), ('want', 246), ('get', 233), ('go', 224), ('like', 223), ('â™›', 210), ('ã€‹', 210), (\"can't\", 180), (\"don't\", 176), ('time', 166), ('feel', 158), ('love', 151), ('day', 150), ('one', 150), ('sorri', 149), ('much', 139)]\n"
     ]
    }
   ],
   "source": [
    "print(get_count(neg_tokens).most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d1c777",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Text cleaning in Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f407674a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Digression: `apply()`\n",
    "\n",
    "`apply()` allows you to apply a function on the rows or columns of a `DataFrame` (or a `Series`) without the use of loop. Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "59a7177e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>ps 1</th>\n",
       "      <th>ps 2</th>\n",
       "      <th>ps 3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Harry</td>\n",
       "      <td>78</td>\n",
       "      <td>56</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hermione</td>\n",
       "      <td>92</td>\n",
       "      <td>88</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ron</td>\n",
       "      <td>64</td>\n",
       "      <td>80</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ginny</td>\n",
       "      <td>68</td>\n",
       "      <td>87</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       name  ps 1  ps 2  ps 3\n",
       "0     Harry    78    56    71\n",
       "1  Hermione    92    88    90\n",
       "2       Ron    64    80    49\n",
       "3     Ginny    68    87    59"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "marks = pd.DataFrame({'name': ['Harry', 'Hermione', 'Ron', 'Ginny'], \n",
    "                      'ps 1': [78, 92, 64, 68], 'ps 2': [56, 88, 80, 87],\n",
    "                      'ps 3': [71, 90, 49, 59]})\n",
    "marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e9304e0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ps 1    28\n",
       "ps 2    32\n",
       "ps 3    41\n",
       "dtype: int64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def my_range(x):\n",
    "    return max(x) - min(x)\n",
    "\n",
    "marks[['ps 1', 'ps 2', 'ps 3']].apply(my_range, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "76aa2763",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    22\n",
       "1     4\n",
       "2    31\n",
       "3    28\n",
       "dtype: int64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "marks[['ps 1', 'ps 2', 'ps 3']].apply(lambda x: x.max() - x.min(), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07dd2ab3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Clean the text in pandas\n",
    "\n",
    "We can use apply each of the pre-processing tasks on the text stored in the `pd.DataFrame` with a sequence of `apply()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "808c846d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [#followfriday, @france_int, @pkuchly57, @mili...\n",
       "1       [@lamb2ja, hey, jame, odd, pleas, call, contac...\n",
       "2       [@despiteoffici, listen, last, night, bleed, a...\n",
       "3                                      [@97side, congrat]\n",
       "4       [yeaaaah, yippppi, accnt, verifi, rqst, succee...\n",
       "                              ...                        \n",
       "9995                          [wanna, chang, avi, usanel]\n",
       "9996                                 [puppi, broke, foot]\n",
       "9997                        [where, jaebum, babi, pictur]\n",
       "9998    [mr, ahmad, maslan, cook, https://t.co/arcid31...\n",
       "9999      [@eawoman, hull, support, expect, misser, week]\n",
       "Name: text, Length: 10000, dtype: object"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'].apply(tweet_tokenise).apply(remove_emoticon).\\\n",
    "    apply(remove_punctuation).apply(to_lower).apply(remove_my_sw).apply(stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef4f756",
   "metadata": {},
   "source": [
    "From the code, you can see the order of pre-processing tasks easily: `tweet_tokenise()` -> `remove_emoticon()` -> `remove_punctuation()` -> `to_lower()` -> `remove_my_sw()` -> `stem()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05454129",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Clean the text in pandas (continue)\n",
    "\n",
    "Alternatively, we can apply the whole pre-processing through the function `get_cleaned_tokens_v3()` (note for v3 we remove @xxx and url as well):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5a07e8ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweet length</th>\n",
       "      <th>cleaned text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#FollowFriday @France_Inte @PKuchly57 @Milipol...</td>\n",
       "      <td>pos</td>\n",
       "      <td>15</td>\n",
       "      <td>#followfriday top engag member communiti week</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@Lamb2ja Hey James! How odd :/ Please call our...</td>\n",
       "      <td>pos</td>\n",
       "      <td>24</td>\n",
       "      <td>hey jame odd pleas call contact centr 02392441...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@DespiteOfficial we had a listen last night :)...</td>\n",
       "      <td>pos</td>\n",
       "      <td>20</td>\n",
       "      <td>listen last night bleed amaz track scotland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@97sides CONGRATS :)</td>\n",
       "      <td>pos</td>\n",
       "      <td>3</td>\n",
       "      <td>congrat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>yeaaaah yippppy!!!  my accnt verified rqst has...</td>\n",
       "      <td>pos</td>\n",
       "      <td>21</td>\n",
       "      <td>yeaaaah yippppi accnt verifi rqst succeed got ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>I wanna change my avi but uSanele :(</td>\n",
       "      <td>neg</td>\n",
       "      <td>8</td>\n",
       "      <td>wanna chang avi usanel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>MY PUPPY BROKE HER FOOT :(</td>\n",
       "      <td>neg</td>\n",
       "      <td>6</td>\n",
       "      <td>puppi broke foot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>where's all the jaebum baby pictures :((</td>\n",
       "      <td>neg</td>\n",
       "      <td>7</td>\n",
       "      <td>where jaebum babi pictur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>But but Mr Ahmad Maslan cooks too :( https://t...</td>\n",
       "      <td>neg</td>\n",
       "      <td>9</td>\n",
       "      <td>mr ahmad maslan cook</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>@eawoman As a Hull supporter I am expecting a ...</td>\n",
       "      <td>neg</td>\n",
       "      <td>13</td>\n",
       "      <td>hull support expect misser week</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text sentiment  \\\n",
       "0     #FollowFriday @France_Inte @PKuchly57 @Milipol...       pos   \n",
       "1     @Lamb2ja Hey James! How odd :/ Please call our...       pos   \n",
       "2     @DespiteOfficial we had a listen last night :)...       pos   \n",
       "3                                  @97sides CONGRATS :)       pos   \n",
       "4     yeaaaah yippppy!!!  my accnt verified rqst has...       pos   \n",
       "...                                                 ...       ...   \n",
       "9995               I wanna change my avi but uSanele :(       neg   \n",
       "9996                         MY PUPPY BROKE HER FOOT :(       neg   \n",
       "9997           where's all the jaebum baby pictures :((       neg   \n",
       "9998  But but Mr Ahmad Maslan cooks too :( https://t...       neg   \n",
       "9999  @eawoman As a Hull supporter I am expecting a ...       neg   \n",
       "\n",
       "      tweet length                                       cleaned text  \n",
       "0               15      #followfriday top engag member communiti week  \n",
       "1               24  hey jame odd pleas call contact centr 02392441...  \n",
       "2               20        listen last night bleed amaz track scotland  \n",
       "3                3                                            congrat  \n",
       "4               21  yeaaaah yippppi accnt verifi rqst succeed got ...  \n",
       "...            ...                                                ...  \n",
       "9995             8                             wanna chang avi usanel  \n",
       "9996             6                                   puppi broke foot  \n",
       "9997             7                           where jaebum babi pictur  \n",
       "9998             9                               mr ahmad maslan cook  \n",
       "9999            13                    hull support expect misser week  \n",
       "\n",
       "[10000 rows x 4 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['cleaned text'] = df['text'].apply(get_cleaned_tokens_v3).str.join(' ')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a59de18",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Clean the text in pandas (continue)\n",
    "\n",
    "Advantages of this approach:\n",
    "* You can keep all your data in a `DataFrame`, instead of having some data stored in some lists\n",
    "* As your data is in `DataFrame`, you can leverage the functionality provided by `pandas`\n",
    "    * e.g. plotting, filtering\n",
    "* Code can be easier to read and easier to modify\n",
    "    * Order of the tasks very clear\n",
    "    \n",
    "(However, counting the number of appearances of each word is trickier when the strings are stored in `DataFrame`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d39134",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Build a model to predict category (demo only)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1ae61e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Objectives\n",
    "\n",
    "* Show NLP (naive and generic) pre-processing does not necessarily improve the performance of models\n",
    "* Show how we can process and extract meaning from emoticons\n",
    "* Show you can build a \"very good performing\" model if you ignore how the data was collected\n",
    "\n",
    "You do NOT need to know how to write the code in this session - the code here is for illustrative purpose only."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ccdb36",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Create \"benchmark\" \n",
    "\n",
    "Here we create the \"naive text\", for which no pre-processing is done on the tweets except removing the emoticons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "05b91cd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweet length</th>\n",
       "      <th>cleaned text</th>\n",
       "      <th>naive text</th>\n",
       "      <th>emoticon text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#FollowFriday @France_Inte @PKuchly57 @Milipol...</td>\n",
       "      <td>pos</td>\n",
       "      <td>15</td>\n",
       "      <td>#followfriday top engag member communiti week</td>\n",
       "      <td>#FollowFriday @France_Inte @PKuchly57 @Milipol...</td>\n",
       "      <td>#followfriday top engag member communiti week ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@Lamb2ja Hey James! How odd :/ Please call our...</td>\n",
       "      <td>pos</td>\n",
       "      <td>24</td>\n",
       "      <td>hey jame odd pleas call contact centr 02392441...</td>\n",
       "      <td>@Lamb2ja Hey James ! How odd :/ Please call ou...</td>\n",
       "      <td>hey jame odd skeptical, annoyed, undecided, un...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@DespiteOfficial we had a listen last night :)...</td>\n",
       "      <td>pos</td>\n",
       "      <td>20</td>\n",
       "      <td>listen last night bleed amaz track scotland</td>\n",
       "      <td>@DespiteOfficial we had a listen last night As...</td>\n",
       "      <td>listen last night happy face or smiley bleed a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@97sides CONGRATS :)</td>\n",
       "      <td>pos</td>\n",
       "      <td>3</td>\n",
       "      <td>congrat</td>\n",
       "      <td>@97sides CONGRATS</td>\n",
       "      <td>congrat happy face or smiley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>yeaaaah yippppy!!!  my accnt verified rqst has...</td>\n",
       "      <td>pos</td>\n",
       "      <td>21</td>\n",
       "      <td>yeaaaah yippppi accnt verifi rqst succeed got ...</td>\n",
       "      <td>yeaaaah yippppy ! ! ! my accnt verified rqst h...</td>\n",
       "      <td>yeaaaah yippppi accnt verifi rqst succeed got ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>I wanna change my avi but uSanele :(</td>\n",
       "      <td>neg</td>\n",
       "      <td>8</td>\n",
       "      <td>wanna chang avi usanel</td>\n",
       "      <td>I wanna change my avi but uSanele</td>\n",
       "      <td>wanna chang avi usanel frown, sad, andry or pout</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>MY PUPPY BROKE HER FOOT :(</td>\n",
       "      <td>neg</td>\n",
       "      <td>6</td>\n",
       "      <td>puppi broke foot</td>\n",
       "      <td>MY PUPPY BROKE HER FOOT</td>\n",
       "      <td>puppi broke foot frown, sad, andry or pout</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>where's all the jaebum baby pictures :((</td>\n",
       "      <td>neg</td>\n",
       "      <td>7</td>\n",
       "      <td>where jaebum babi pictur</td>\n",
       "      <td>where's all the jaebum baby pictures (</td>\n",
       "      <td>where jaebum babi pictur frown, sad, andry or ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>But but Mr Ahmad Maslan cooks too :( https://t...</td>\n",
       "      <td>neg</td>\n",
       "      <td>9</td>\n",
       "      <td>mr ahmad maslan cook</td>\n",
       "      <td>But but Mr Ahmad Maslan cooks too https://t.co...</td>\n",
       "      <td>mr ahmad maslan cook frown, sad, andry or pout</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>@eawoman As a Hull supporter I am expecting a ...</td>\n",
       "      <td>neg</td>\n",
       "      <td>13</td>\n",
       "      <td>hull support expect misser week</td>\n",
       "      <td>@eawoman As a Hull supporter I am expecting a ...</td>\n",
       "      <td>hull support expect misser week frown, sad, an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text sentiment  \\\n",
       "0     #FollowFriday @France_Inte @PKuchly57 @Milipol...       pos   \n",
       "1     @Lamb2ja Hey James! How odd :/ Please call our...       pos   \n",
       "2     @DespiteOfficial we had a listen last night :)...       pos   \n",
       "3                                  @97sides CONGRATS :)       pos   \n",
       "4     yeaaaah yippppy!!!  my accnt verified rqst has...       pos   \n",
       "...                                                 ...       ...   \n",
       "9995               I wanna change my avi but uSanele :(       neg   \n",
       "9996                         MY PUPPY BROKE HER FOOT :(       neg   \n",
       "9997           where's all the jaebum baby pictures :((       neg   \n",
       "9998  But but Mr Ahmad Maslan cooks too :( https://t...       neg   \n",
       "9999  @eawoman As a Hull supporter I am expecting a ...       neg   \n",
       "\n",
       "      tweet length                                       cleaned text  \\\n",
       "0               15      #followfriday top engag member communiti week   \n",
       "1               24  hey jame odd pleas call contact centr 02392441...   \n",
       "2               20        listen last night bleed amaz track scotland   \n",
       "3                3                                            congrat   \n",
       "4               21  yeaaaah yippppi accnt verifi rqst succeed got ...   \n",
       "...            ...                                                ...   \n",
       "9995             8                             wanna chang avi usanel   \n",
       "9996             6                                   puppi broke foot   \n",
       "9997             7                           where jaebum babi pictur   \n",
       "9998             9                               mr ahmad maslan cook   \n",
       "9999            13                    hull support expect misser week   \n",
       "\n",
       "                                             naive text  \\\n",
       "0     #FollowFriday @France_Inte @PKuchly57 @Milipol...   \n",
       "1     @Lamb2ja Hey James ! How odd :/ Please call ou...   \n",
       "2     @DespiteOfficial we had a listen last night As...   \n",
       "3                                     @97sides CONGRATS   \n",
       "4     yeaaaah yippppy ! ! ! my accnt verified rqst h...   \n",
       "...                                                 ...   \n",
       "9995                  I wanna change my avi but uSanele   \n",
       "9996                            MY PUPPY BROKE HER FOOT   \n",
       "9997             where's all the jaebum baby pictures (   \n",
       "9998  But but Mr Ahmad Maslan cooks too https://t.co...   \n",
       "9999  @eawoman As a Hull supporter I am expecting a ...   \n",
       "\n",
       "                                          emoticon text  \n",
       "0     #followfriday top engag member communiti week ...  \n",
       "1     hey jame odd skeptical, annoyed, undecided, un...  \n",
       "2     listen last night happy face or smiley bleed a...  \n",
       "3                          congrat happy face or smiley  \n",
       "4     yeaaaah yippppi accnt verifi rqst succeed got ...  \n",
       "...                                                 ...  \n",
       "9995   wanna chang avi usanel frown, sad, andry or pout  \n",
       "9996         puppi broke foot frown, sad, andry or pout  \n",
       "9997  where jaebum babi pictur frown, sad, andry or ...  \n",
       "9998     mr ahmad maslan cook frown, sad, andry or pout  \n",
       "9999  hull support expect misser week frown, sad, an...  \n",
       "\n",
       "[10000 rows x 6 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['naive text'] = df['text'].apply(tweet_tokenise).apply(remove_emoticon).str.join(' ')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798547ba",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Convert text to a matrix of TF-IDF features (demo only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2f30ae36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b63171a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_text_tf = TfidfVectorizer().fit_transform(df[\"naive text\"])\n",
    "cleaned_text_tf = TfidfVectorizer().fit_transform(df[\"cleaned text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "dc17f37d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<10000x20788 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 97915 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive_text_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a9fdfd4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<10000x10051 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 53761 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_text_tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b9f8ed",
   "metadata": {},
   "source": [
    "Why sparse matrix? We will talk about it in week 9. Stay tuned!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf270af",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Train the model: without NLP preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "57f60a76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.774\n"
     ]
    }
   ],
   "source": [
    "# you do not need to understand the code here\n",
    "naive_X_train, naive_X_test, naive_y_train, naive_y_test = train_test_split(\n",
    "    naive_text_tf, df[\"sentiment\"], test_size=0.3, random_state=1\n",
    ")\n",
    "\n",
    "clf = MultinomialNB().fit(naive_X_train, naive_y_train)\n",
    "predicted = clf.predict(naive_X_test)\n",
    "\n",
    "print(\"Accuracy:\", metrics.accuracy_score(naive_y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7448d50b",
   "metadata": {},
   "source": [
    "This shows that using the text _without_ pre-processing, 77.4% of the time the model classifies the category (positive / negative) correctly with the given \"test\" data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e924c80",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Train the model: with NLP preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fc3eb778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.75\n"
     ]
    }
   ],
   "source": [
    "# you do not need to understand the code here\n",
    "cleaned_X_train, cleaned_X_test, cleaned_y_train, cleaned_y_test = train_test_split(\n",
    "    cleaned_text_tf, df[\"sentiment\"], test_size=0.3, random_state=1\n",
    ")\n",
    "\n",
    "clf = MultinomialNB().fit(cleaned_X_train, cleaned_y_train)\n",
    "predicted = clf.predict(cleaned_X_test)\n",
    "\n",
    "print(\"Accuracy:\", metrics.accuracy_score(cleaned_y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ff6448",
   "metadata": {},
   "source": [
    "This shows that using the text _with_ processing, 75% of the time the model classifies the category (positive / negative) correctly with the given \"test\" data. \n",
    "* No evidence that NLP pre-processing improves the result of the model for this example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab876496",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pre-processing or not?\n",
    "\n",
    "* For exploration purpose, we have seen pre-processing is useful - consider what you have done in PS2. Without removing the stop words, you are not able to get a useful result\n",
    "* When modeling data, how to pre-process the data depending on what model you are using. For example, some models take care of stop words automatically, so it may be better not to remove them directly by yourself\n",
    "    * For example, when classifying the category of text, some models automatically give low weights to words appear frequently in different categories (which includes the stop words), and provide high weights to words that appear frequently only in a specific category"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5f2452",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Extracting meaning from emoticons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b9c0f45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note previously I used EMOTICONS instead of EMOTICONS_EMO\n",
    "# it is because I was using emot in version 2 - they have decided to use another name \n",
    "# for the same thing in the new version 3...\n",
    "from emot.emo_unicode import EMOTICONS_EMO # you may need to install emot\n",
    "\n",
    "my_emot = {emot.replace('\\\\', ''): txt for emot, txt in EMOTICONS_EMO.items()}\n",
    "\n",
    "def convert_emoticon(tokens):\n",
    "    # convert emoticon to text\n",
    "    func = lambda token: my_emot[token] if token in my_emot else token\n",
    "    return [func(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132f2d0f",
   "metadata": {},
   "source": [
    "Example use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4119458a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Happy face or smiley', 'Happy face smiley', 'Frown, sad, andry or pouting']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_emoticon([':)', ':-)', ':('])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102a7c65",
   "metadata": {},
   "source": [
    "By turning the emoticons into text, it helps us to \"group\" similar emoticons together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5348d4a7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Extracting meaning from emoticons (continue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e5ed8114",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cleaned_tokens_v4(text):\n",
    "    # now emoticons are converted to text\n",
    "    tokens = tweet_tokenise(text)\n",
    "    tokens = remove_start_with(tokens, '@')\n",
    "    tokens = remove_start_with(tokens, 'http')\n",
    "    tokens = convert_emoticon(tokens) \n",
    "    tokens = remove_punctuation(tokens)\n",
    "    tokens = to_lower(tokens)\n",
    "    tokens = remove_my_sw(tokens)\n",
    "    tokens = stem(tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c302fe1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Extracting meaning from emoticon (continue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "02f83384",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweet length</th>\n",
       "      <th>cleaned text</th>\n",
       "      <th>naive text</th>\n",
       "      <th>emoticon text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#FollowFriday @France_Inte @PKuchly57 @Milipol...</td>\n",
       "      <td>pos</td>\n",
       "      <td>15</td>\n",
       "      <td>#followfriday top engag member communiti week</td>\n",
       "      <td>#FollowFriday @France_Inte @PKuchly57 @Milipol...</td>\n",
       "      <td>#followfriday top engag member communiti week ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@Lamb2ja Hey James! How odd :/ Please call our...</td>\n",
       "      <td>pos</td>\n",
       "      <td>24</td>\n",
       "      <td>hey jame odd pleas call contact centr 02392441...</td>\n",
       "      <td>@Lamb2ja Hey James ! How odd :/ Please call ou...</td>\n",
       "      <td>hey jame odd skeptical, annoyed, undecided, un...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@DespiteOfficial we had a listen last night :)...</td>\n",
       "      <td>pos</td>\n",
       "      <td>20</td>\n",
       "      <td>listen last night bleed amaz track scotland</td>\n",
       "      <td>@DespiteOfficial we had a listen last night As...</td>\n",
       "      <td>listen last night happy face or smiley bleed a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@97sides CONGRATS :)</td>\n",
       "      <td>pos</td>\n",
       "      <td>3</td>\n",
       "      <td>congrat</td>\n",
       "      <td>@97sides CONGRATS</td>\n",
       "      <td>congrat happy face or smiley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>yeaaaah yippppy!!!  my accnt verified rqst has...</td>\n",
       "      <td>pos</td>\n",
       "      <td>21</td>\n",
       "      <td>yeaaaah yippppi accnt verifi rqst succeed got ...</td>\n",
       "      <td>yeaaaah yippppy ! ! ! my accnt verified rqst h...</td>\n",
       "      <td>yeaaaah yippppi accnt verifi rqst succeed got ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>I wanna change my avi but uSanele :(</td>\n",
       "      <td>neg</td>\n",
       "      <td>8</td>\n",
       "      <td>wanna chang avi usanel</td>\n",
       "      <td>I wanna change my avi but uSanele</td>\n",
       "      <td>wanna chang avi usanel frown, sad, andry or pout</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>MY PUPPY BROKE HER FOOT :(</td>\n",
       "      <td>neg</td>\n",
       "      <td>6</td>\n",
       "      <td>puppi broke foot</td>\n",
       "      <td>MY PUPPY BROKE HER FOOT</td>\n",
       "      <td>puppi broke foot frown, sad, andry or pout</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>where's all the jaebum baby pictures :((</td>\n",
       "      <td>neg</td>\n",
       "      <td>7</td>\n",
       "      <td>where jaebum babi pictur</td>\n",
       "      <td>where's all the jaebum baby pictures (</td>\n",
       "      <td>where jaebum babi pictur frown, sad, andry or ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>But but Mr Ahmad Maslan cooks too :( https://t...</td>\n",
       "      <td>neg</td>\n",
       "      <td>9</td>\n",
       "      <td>mr ahmad maslan cook</td>\n",
       "      <td>But but Mr Ahmad Maslan cooks too https://t.co...</td>\n",
       "      <td>mr ahmad maslan cook frown, sad, andry or pout</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>@eawoman As a Hull supporter I am expecting a ...</td>\n",
       "      <td>neg</td>\n",
       "      <td>13</td>\n",
       "      <td>hull support expect misser week</td>\n",
       "      <td>@eawoman As a Hull supporter I am expecting a ...</td>\n",
       "      <td>hull support expect misser week frown, sad, an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text sentiment  \\\n",
       "0     #FollowFriday @France_Inte @PKuchly57 @Milipol...       pos   \n",
       "1     @Lamb2ja Hey James! How odd :/ Please call our...       pos   \n",
       "2     @DespiteOfficial we had a listen last night :)...       pos   \n",
       "3                                  @97sides CONGRATS :)       pos   \n",
       "4     yeaaaah yippppy!!!  my accnt verified rqst has...       pos   \n",
       "...                                                 ...       ...   \n",
       "9995               I wanna change my avi but uSanele :(       neg   \n",
       "9996                         MY PUPPY BROKE HER FOOT :(       neg   \n",
       "9997           where's all the jaebum baby pictures :((       neg   \n",
       "9998  But but Mr Ahmad Maslan cooks too :( https://t...       neg   \n",
       "9999  @eawoman As a Hull supporter I am expecting a ...       neg   \n",
       "\n",
       "      tweet length                                       cleaned text  \\\n",
       "0               15      #followfriday top engag member communiti week   \n",
       "1               24  hey jame odd pleas call contact centr 02392441...   \n",
       "2               20        listen last night bleed amaz track scotland   \n",
       "3                3                                            congrat   \n",
       "4               21  yeaaaah yippppi accnt verifi rqst succeed got ...   \n",
       "...            ...                                                ...   \n",
       "9995             8                             wanna chang avi usanel   \n",
       "9996             6                                   puppi broke foot   \n",
       "9997             7                           where jaebum babi pictur   \n",
       "9998             9                               mr ahmad maslan cook   \n",
       "9999            13                    hull support expect misser week   \n",
       "\n",
       "                                             naive text  \\\n",
       "0     #FollowFriday @France_Inte @PKuchly57 @Milipol...   \n",
       "1     @Lamb2ja Hey James ! How odd :/ Please call ou...   \n",
       "2     @DespiteOfficial we had a listen last night As...   \n",
       "3                                     @97sides CONGRATS   \n",
       "4     yeaaaah yippppy ! ! ! my accnt verified rqst h...   \n",
       "...                                                 ...   \n",
       "9995                  I wanna change my avi but uSanele   \n",
       "9996                            MY PUPPY BROKE HER FOOT   \n",
       "9997             where's all the jaebum baby pictures (   \n",
       "9998  But but Mr Ahmad Maslan cooks too https://t.co...   \n",
       "9999  @eawoman As a Hull supporter I am expecting a ...   \n",
       "\n",
       "                                          emoticon text  \n",
       "0     #followfriday top engag member communiti week ...  \n",
       "1     hey jame odd skeptical, annoyed, undecided, un...  \n",
       "2     listen last night happy face or smiley bleed a...  \n",
       "3                          congrat happy face or smiley  \n",
       "4     yeaaaah yippppi accnt verifi rqst succeed got ...  \n",
       "...                                                 ...  \n",
       "9995   wanna chang avi usanel frown, sad, andry or pout  \n",
       "9996         puppi broke foot frown, sad, andry or pout  \n",
       "9997  where jaebum babi pictur frown, sad, andry or ...  \n",
       "9998     mr ahmad maslan cook frown, sad, andry or pout  \n",
       "9999  hull support expect misser week frown, sad, an...  \n",
       "\n",
       "[10000 rows x 6 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['emoticon text'] = df['text'].apply(get_cleaned_tokens_v4).str.join(' ')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ca8eb8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Modeling with meaning of emoticons extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "376b3b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7746666666666666\n"
     ]
    }
   ],
   "source": [
    "# base case\n",
    "text_tf = TfidfVectorizer().fit_transform(df[\"text\"])\n",
    "\n",
    "text_X_train, text_X_test, text_y_train, text_y_test = train_test_split(\n",
    "    text_tf, df[\"sentiment\"], test_size=0.3, random_state=1\n",
    ")\n",
    "\n",
    "clf = MultinomialNB().fit(text_X_train, text_y_train)\n",
    "predicted = clf.predict(text_X_test)\n",
    "\n",
    "print(\"Accuracy:\", metrics.accuracy_score(text_y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ad3bb57e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.99\n"
     ]
    }
   ],
   "source": [
    "# with pre-processing\n",
    "emoticon_text_tf = TfidfVectorizer().fit_transform(df[\"emoticon text\"])\n",
    "\n",
    "emoticon_X_train, emoticon_X_test, emoticon_y_train, emoticon_y_test = train_test_split(\n",
    "    emoticon_text_tf, df[\"sentiment\"], test_size=0.3, random_state=1\n",
    ")\n",
    "\n",
    "clf = MultinomialNB().fit(emoticon_X_train, emoticon_y_train)\n",
    "predicted = clf.predict(emoticon_X_test)\n",
    "\n",
    "print(\"Accuracy:\", metrics.accuracy_score(emoticon_y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e70ea02",
   "metadata": {},
   "source": [
    "Now we have a model with very high accuracy - we are 99% correct. Shame this only \"works\" because of how the data was collected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a0f464",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "* Manipulate tweets in json format\n",
    "* NLP application\n",
    "    * Explore different tokenisers\n",
    "    * Consider the list of stop words to use\n",
    "    * Convert emoticons to text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe361f87",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0830856d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## List comprehension\n",
    "List comprehension offers a way to create a new list when each element is the result of some operations on the element of another sequence.\n",
    "* For example, if we want to create a list to store the marks of each students with the marks adjusted by 3 points, we could:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a81435",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_1 = [75, 92, 64]\n",
    "new_marks = []\n",
    "for mark in ps_1:\n",
    "    new_marks.append(mark+3)\n",
    "new_marks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5478d30",
   "metadata": {},
   "source": [
    "But we can rewrite it using list comprehension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e257f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "[mark + 3 for mark in ps_1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a03f27",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lambda function\n",
    "\n",
    "* Lambda function (or anonymous function) is a way of writing functions consisting of a single statement\n",
    "* Syntax: `lambda x: return_value`\n",
    "* For example, the function `mean()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cf32db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_range(x):\n",
    "    return max(x) - min(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0453f0f4",
   "metadata": {},
   "source": [
    "Can be rewritten using lambda function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6091b51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_range = lambda x: max(x) - min(x)\n",
    "type(my_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c4c0a9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Use a lambda function\n",
    "\n",
    "It is just a normal function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50990d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [1, 2, 3, 4, 5]\n",
    "my_range(x)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
